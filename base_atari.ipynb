{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium[atari]\n",
    "# !pip install gymnasium matplotlib numpy\n",
    "# !pip install ale-py\n",
    "# !pip install gymnasium[accept-rom-license]\n",
    "# !pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ale_py import ALEInterface\n",
    "from ale_py.roms import Breakout\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ale = ALEInterface()\n",
    "ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(frame): \n",
    "    shape = frame.shape\n",
    "    grayscale = np.zeros(shape=(shape[0], shape[1]))\n",
    "    for x in range(shape[0]):\n",
    "      for y in range(shape[1]):\n",
    "        grayscale[x, y] = np.sum(frame[x][y])/3\n",
    "    \n",
    "    return grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\") #, render_mode=\"human\"\n",
    "observation, info = env.reset()\n",
    "np.set_printoptions(threshold = np.inf)\n",
    "\n",
    "next_frame, reward, terminated, truncated, info = env.step(1)\n",
    "print('Reward Recieved = ' + str(reward))\n",
    "print('Next state is a terminal state: ' + str(terminated))\n",
    "print('info[ale.lives] tells us how many lives we have. Lives: ' + str(info['lives']))\n",
    "print(env.reward_range)\n",
    "\n",
    "for i in range(10000):\n",
    "    a = 3 #random.sample([0,1,2,3] , 1)[0]\n",
    "    next_frame, reward, terminated, truncated, info = env.step(a)\n",
    "    print(next_frame)\n",
    "    print(\"hey\")\n",
    "    env.render()\n",
    "    if truncated == True:\n",
    "        env.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeps track of the agents memory\n",
    "def Agent_memory():\n",
    "    def __init__(self):\n",
    "        self.action_memory = []\n",
    "        self.frame_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.next_state_memory = []\n",
    "#function to keep adding to the memory\n",
    "    def add_memory(self, next_frame, next_frames_reward, next_action):\n",
    "        self.frame_memory.append(next_frame)\n",
    "        self.action_memory.append(next_action)\n",
    "        self.reward_memory.append(next_frames_reward)\n",
    "        #self.done_flags.append(next_frame_terminal)\n",
    "        \n",
    "# s to s-, need ot know all possible values of s dash and the probablity matrix,\n",
    "# for each state and all possibnle acto what is the prbablity of going to next staticmethod\n",
    "#     function promixater, you tell it what state, what action u are taking and tell u what the value of ur next state is\n",
    "# that has to be train, the function promixer is the NN, action value function. try to find a value action function. \n",
    "#given the state, how can we find these variables, the more state space more complex, x, y, vx\n",
    "#define state space, define actions, we have to define gamma, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epsilon = 0.99\n",
    "gamma = 0.99\n",
    "episodes = 100\n",
    "max_timesteps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, possible_actions, starting_mem_len, max_mem_len, starting_epsilon, learn_rate, starting_lives = 5, debug = False) -> None:\n",
    "       pass\n",
    "    '''gets the next set of actions from what the agent learns'''\n",
    "    def get_action(self):\n",
    "        pass\n",
    "    def learn(self):\n",
    "        pass\n",
    "    def step(self, env, action):\n",
    "        env.step(action)\n",
    "        pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
