{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRLJ5REFU-aY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "ec20916d-d8a0-4bc0-a046-9db28968ed36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[atari])\n",
            "  Using cached Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.1.0)\n",
            "Installing collected packages: shimmy\n",
            "  Attempting uninstall: shimmy\n",
            "    Found existing installation: Shimmy 1.1.0\n",
            "    Uninstalling Shimmy-1.1.0:\n",
            "      Successfully uninstalled Shimmy-1.1.0\n",
            "Successfully installed shimmy-0.2.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "shimmy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium matplotlib numpy\n",
        "!pip install ale-py\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "!pip install tensorflow\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install opencv-python\n",
        "!pip install git+https://github.com/openai/baselines\n",
        "!pip install pandas\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwYTFl7lU-ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "27035b40-c5b5-4b0c-8e7f-dc2d353031da"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d403eac09cd0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0male_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALEInterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0male_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBreakout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgymnasium\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ale_py'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from ale_py import ALEInterface\n",
        "from ale_py.roms import Breakout\n",
        "import gymnasium as gym\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "!python -m atari_py.import_roms /content/Roms.rar\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "id": "EgBO5s40tRwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBXfXjGIU-ad"
      },
      "outputs": [],
      "source": [
        "ale = ALEInterface()\n",
        "ale.loadROM(Breakout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCeKcH_HU-ad"
      },
      "outputs": [],
      "source": [
        "def convert_to_grayscale(frame):\n",
        "    height, width = frame.shape[:2]\n",
        "    shape = frame.shape\n",
        "    grayscale = np.zeros(shape=(height, width))\n",
        "    for x in range(shape[0]):\n",
        "      for y in range(shape[1]):\n",
        "        grayscale[x, y] = np.sum(frame[x][y])/3\n",
        "\n",
        "    #rescales the frame\n",
        "    res = cv2.resize(grayscale,(84, 84), interpolation = cv2.INTER_AREA)\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTyo8gytU-ae"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\") #, render_mode=\"human\"\n",
        "# observation, info = env.reset()\n",
        "# np.set_printoptions(threshold = np.inf)\n",
        "\n",
        "# next_frame, reward, terminated, truncated, info = env.step(1)\n",
        "# print('Reward Recieved = ' + str(reward))\n",
        "# print('Next state is a terminal state: ' + str(terminated))\n",
        "# print('info[ale.lives] tells us how many lives we have. Lives: ' + str(info['lives']))\n",
        "# print(env.reward_range)\n",
        "\n",
        "# for i in range(10000):\n",
        "#     a = 3 #random.sample([0,1,2,3] , 1)[0]\n",
        "#     next_frame, reward, terminated, truncated, info = env.step(a)\n",
        "#     print(next_frame)\n",
        "#     print(\"hey\")\n",
        "#     env.render()\n",
        "#     if truncated == True:\n",
        "#         env.reset()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class QNetwork(nn.Module):\n",
        "#     def __init__(self, env):\n",
        "#         super().__init__()\n",
        "#         self.network = nn.Sequential(\n",
        "#             nn.Conv2d(4, 32, 8, stride=4),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(32, 64, 4, stride=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(64, 64, 3, stride=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Flatten(),\n",
        "#             nn.Linear(3136, 512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(512, env.single_action_space.n),\n",
        "#         )\n",
        "#     def forward(self, x):\n",
        "#         return self.network(x / 255.0)"
      ],
      "metadata": {
        "id": "8yGNPksW_wZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "    slope = (end_e - start_e) / duration\n",
        "    return max(slope * t + start_e, end_e)"
      ],
      "metadata": {
        "id": "l4MfzVZa_1B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CustomAtariWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(CustomAtariWrapper, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(0, 255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        observation = self.env.reset(**kwargs)\n",
        "        return self.preprocess(observation)\n",
        "\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        return self.preprocess(observation), reward, done, info\n",
        "\n",
        "    def preprocess(self, frame):\n",
        "      height, width = frame.shape[:2]\n",
        "      shape = frame.shape\n",
        "      grayscale = np.zeros(shape=(height, width))\n",
        "      for x in range(shape[0]):\n",
        "        for y in range(shape[1]):\n",
        "          grayscale[x, y] = np.sum(frame[x][y])/3\n",
        "\n",
        "      #rescales the frame\n",
        "      res = cv2.resize(grayscale,(84, 84), interpolation = cv2.INTER_AREA)\n",
        "      return res\n",
        "\n",
        "    # def preprocess(self, observation):\n",
        "    #     # Convert  grayscale\n",
        "    #     grayscale_observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    #     # Resize to 84x84\n",
        "    #     resized_observation = cv2.resize(grayscale_observation, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    #     # Reshape to (84, 84, 1)\n",
        "    #     reshaped_observation = np.expand_dims(resized_observation, axis=-1)\n",
        "\n",
        "    #     return reshaped_observation\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h7BHz7i9JL8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Title: Deep Q-Learning for Atari Breakout\n",
        "Author: [Jacob Chapman](https://twitter.com/jacoblchapman) and [Mathias Lechner](https://twitter.com/MLech20)\n",
        "Date created: 2020/05/23\n",
        "Last modified: 2020/06/17\n",
        "Description: Play Atari Breakout with a Deep Q-Network.\n",
        "Accelerator: NONE\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "## Introduction\n",
        "\n",
        "This script shows an implementation of Deep Q-Learning on the\n",
        "`BreakoutNoFrameskip-v4` environment.\n",
        "\n",
        "This example requires the following dependencies: `baselines`, `atari-py`, `rows`.\n",
        "They can be installed via:\n",
        "\n",
        "```\n",
        "git clone https://github.com/openai/baselines.git\n",
        "cd baselines\n",
        "pip install -e .\n",
        "git clone https://github.com/openai/atari-py\n",
        "wget http://www.atarimania.com/roms/Roms.rar\n",
        "unrar x Roms.rar .\n",
        "python -m atari_py.import_roms .\n",
        "```\n",
        "\n",
        "### Deep Q-Learning\n",
        "\n",
        "As an agent takes actions and moves through an environment, it learns to map\n",
        "the observed state of the environment to an action. An agent will choose an action\n",
        "in a given state based on a \"Q-value\", which is a weighted reward based on the\n",
        "expected highest long-term reward. A Q-Learning Agent learns to perform its\n",
        "task such that the recommended action maximizes the potential future rewards.\n",
        "This method is considered an \"Off-Policy\" method,\n",
        "meaning its Q values are updated assuming that the best action was chosen, even\n",
        "if the best action was not chosen.\n",
        "\n",
        "### Atari Breakout\n",
        "\n",
        "In this environment, a board moves along the bottom of the screen returning a ball that\n",
        "will destroy blocks at the top of the screen.\n",
        "The aim of the game is to remove all blocks and breakout of the\n",
        "level. The agent must learn to control the board by moving left and right, returning the\n",
        "ball and removing all the blocks without the ball passing the board.\n",
        "\n",
        "### Note\n",
        "\n",
        "The Deepmind paper trained for \"a total of 50 million frames (that is, around 38 days of\n",
        "game experience in total)\". However this script will give good results at around 10\n",
        "million frames which are processed in less than 24 hours on a modern machine.\n",
        "\n",
        "### References\n",
        "\n",
        "- [Q-Learning](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf)\n",
        "- [Deep Q-Learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "## Setup\n",
        "\"\"\"\n",
        "\n",
        "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from collections import deque\n",
        "\n",
        "reward_loss = pd.DataFrame(columns=['reward', 'loss'])\n",
        "# Configuration paramaters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
        "epsilon_interval = (\n",
        "    epsilon_max - epsilon_min\n",
        ")  # Rate at which to reduce chance of random action being taken\n",
        "batch_size = 256  # Size of batch taken from replay buffer\n",
        "max_steps_per_episode = 10000\n",
        "\n",
        "env = gym.make(\"BreakoutNoFrameskip-v4m\") #, render_mode=\"human\"\n",
        "\n",
        "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\n",
        "env = gym.wrappers.AtariPreprocessing(env)\n",
        "\n",
        "env.seed(seed)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Implement the Deep Q-Network\n",
        "\n",
        "This network learns an approximation of the Q-table, which is a mapping between\n",
        "the states and actions that an agent will take. For every state we'll have four\n",
        "actions, that can be taken. The environment provides the state, and the action\n",
        "is chosen by selecting the larger of the four Q-values predicted in the output layer.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "num_actions = 4\n",
        "\n",
        "\n",
        "def create_q_model():\n",
        "\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = layers.Input(\n",
        "        shape=(\n",
        "            84,\n",
        "            84,\n",
        "            4,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Convolutions on the frames on the screen\n",
        "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
        "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
        "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
        "\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "\n",
        "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "\n",
        "# The first model makes the predictions for Q-values which are used to\n",
        "# make a action.\n",
        "model = create_q_model()\n",
        "# Build a target model for the prediction of future rewards.\n",
        "# The weights of a target model get updated every 10000 steps thus when the\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\n",
        "model_target = create_q_model()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Train\n",
        "\"\"\"\n",
        "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "# improves training time\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
        "\n",
        "# Experience replay buffers\n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0\n",
        "# Number of frames to take random action and observe output\n",
        "epsilon_random_frames = 50000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 1000000.0\n",
        "# Maximum replay length\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
        "max_memory_length = 100000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 10000\n",
        "# Using huber loss for stability\n",
        "loss_function = keras.losses.Huber()\n",
        "\n",
        "loss_tracker = []\n",
        "save_count = 0\n",
        "#if loss less than minimun of this list then save model\n",
        "loss = None\n",
        "while True:  # Run until solved\n",
        "    state = np.array(env.reset())\n",
        "\n",
        "    episode_reward = 0\n",
        "    frame_stack = deque(maxlen=4)\n",
        "    frame_stack_state_sample = deque(maxlen=4)\n",
        "    count = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "\n",
        "        # env.render(); Adding this line would show the attempts\n",
        "        # of the agent in a pop up window.\n",
        "        frame_count += 1\n",
        "\n",
        "\n",
        "        # Use epsilon-greedy for exploration\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
        "            # Take random action\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            # Predict action Q-values\n",
        "            # From environment state\n",
        "            state_tensor = tf.convert_to_tensor(state)\n",
        "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "            action_probs = model(state_tensor, training=False)\n",
        "            # Take best action\n",
        "            action = tf.argmax(action_probs[0]).numpy()\n",
        "\n",
        "        # Decay probability of taking random action\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "        epsilon = max(epsilon, epsilon_min)\n",
        "\n",
        "        # Apply the sampled action in our environment\n",
        "        state_next, reward, done, trunk, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "        frame_stack.append(state_next)\n",
        "\n",
        "\n",
        "        count+=1\n",
        "        if(count == 4):\n",
        "          count = 0\n",
        "          state_next = np.array(state_next)\n",
        "          #print(state_next.shape)\n",
        "\n",
        "\n",
        "          # Save actions and states in replay buffer\n",
        "          action_history.append(action)\n",
        "          #state_history.append(state)\n",
        "\n",
        "          frame_stack.append(state_next)\n",
        "          stacked_state = np.stack(frame_stack, axis=-1)\n",
        "\n",
        "          frame_stack_state_sample.append(state)\n",
        "          state_his_stack = np.stack(frame_stack, axis=-1)\n",
        "\n",
        "\n",
        "          state_next_history.append(stacked_state)\n",
        "          state_history.append(state_his_stack)\n",
        "\n",
        "          #print(stacked_state.shape)\n",
        "\n",
        "\n",
        "          done_history.append(done)\n",
        "          rewards_history.append(reward)\n",
        "          state = state_next\n",
        "          #print(len(state_next_history))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Update every fourth frame and once batch size is over 32\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "            #state_next_history.append(stacked_state)\n",
        "            #frame_stack.clear()\n",
        "\n",
        "            # Get indices of samples for replay buffers\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "            # Using list comprehension to sample from replay buffer\n",
        "            state_sample = np.array([state_history[i] for i in indices])\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
        "            rewards_sample = [rewards_history[i] for i in indices]\n",
        "            action_sample = [action_history[i] for i in indices]\n",
        "            done_sample = tf.convert_to_tensor(\n",
        "                [float(done_history[i]) for i in indices]\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "            # Build the updated Q-values for the sampled future states\n",
        "            # Use the target model for stability\n",
        "\n",
        "            # state_next_sample = np.reshape(state_next_sample, (batch_size, 84, 84, 4))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            future_rewards = model_target.predict(state_next_sample)\n",
        "            # Q value = reward + discount factor * expected future reward\n",
        "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
        "                future_rewards, axis=1\n",
        "            )\n",
        "\n",
        "            # If final frame set the last value to -1\n",
        "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
        "\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\n",
        "            masks = tf.one_hot(action_sample, num_actions)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Train the model on the states and updated Q-values\n",
        "                state_sample = tf.convert_to_tensor(state_sample, dtype=tf.float32)\n",
        "                #print(state_sample.shape)\n",
        "\n",
        "\n",
        "\n",
        "                q_values = model(state_sample)\n",
        "\n",
        "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "                # Calculate loss between new Q-value and old Q-value\n",
        "                loss = loss_function(updated_q_values, q_action)\n",
        "                loss_tracker.append(loss)\n",
        "\n",
        "            # Backpropagation\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if frame_count % update_target_network == 0:\n",
        "            # update the the target network with new weights\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            # Log details\n",
        "\n",
        "\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
        "            print(template.format(running_reward, episode_count, frame_count))\n",
        "\n",
        "        # Limit the state and reward history\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del done_history[:1]\n",
        "\n",
        "\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Update running reward to check condition for solving\n",
        "    if(loss != None):\n",
        "      print(\"Episode Count \", episode_count, \"EPSIODE_rEWARD:\", episode_reward, \"Loss:\", loss.numpy() )\n",
        "      save_count +=1\n",
        "      s = pd.Series([episode_reward,loss.numpy()],index=['reward', 'loss'], name=save_count)\n",
        "      reward_loss = pd.concat([reward_loss, s.to_frame().T])\n",
        "      reward_loss.to_csv('/content/gdrive/My Drive/dataframe_rl.csv')\n",
        "      print(reward_loss)\n",
        "    if(episode_count % 10 == 0  ):\n",
        "\n",
        "      model.save('/content/gdrive/My Drive/my_model_episode.hdf5')\n",
        "    if len(loss_tracker) != 0 and loss.numpy() < np.min(loss_tracker):\n",
        "        model.save('/content/gdrive/My Drive/my_model.hdf5')\n",
        "\n",
        "    episode_reward_history.append(episode_reward)\n",
        "\n",
        "    if len(episode_reward_history) > 100:\n",
        "        del episode_reward_history[:1]\n",
        "\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "    episode_count += 1\n",
        "\n",
        "    if running_reward > 40:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break\n",
        "\n",
        "\n",
        "#how tf do"
      ],
      "metadata": {
        "id": "vF5hFdrn_-Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y62dW7hKU-ae"
      },
      "outputs": [],
      "source": [
        "#keeps track of the agents memory\n",
        "def Agent_memory():\n",
        "    def __init__(self):\n",
        "        self.action_memory = []\n",
        "        self.frame_memory = []\n",
        "        self.reward_memory = []\n",
        "        self.next_state_memory = []\n",
        "#function to keep adding to the memory\n",
        "    def add_memory(self, next_frame, next_frames_reward, next_action):\n",
        "        self.frame_memory.append(next_frame)\n",
        "        self.action_memory.append(next_action)\n",
        "        self.reward_memory.append(next_frames_reward)\n",
        "        #self.done_flags.append(next_frame_terminal)\n",
        "\n",
        "# s to s-, need ot know all possible values of s dash and the probablity matrix,\n",
        "# for each state and all possibnle acto what is the prbablity of going to next staticmethod\n",
        "#     function promixater, you tell it what state, what action u are taking and tell u what the value of ur next state is\n",
        "# that has to be train, the function promixer is the NN, action value function. try to find a value action function.\n",
        "#given the state, how can we find these variables, the more state space more complex, x, y, vx\n",
        "#define state space, define actions, we have to define gamma,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_rdLEuLU-af"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.5\n",
        "epsilon = 0.99\n",
        "gamma = 0.99\n",
        "episodes = 100\n",
        "max_timesteps = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2gj1bn0U-af"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, possible_actions, starting_mem_len, max_mem_len, starting_epsilon, learn_rate, starting_lives = 5, debug = False) -> None:\n",
        "       pass\n",
        "    '''gets the next set of actions from what the agent learns'''\n",
        "    def get_action(self):\n",
        "        pass\n",
        "    def learn(self):\n",
        "        pass\n",
        "    def step(self, env, action):\n",
        "        env.step(action)\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(self, current_position, enochs, epsilon, gamma):\n",
        "  history = Agent_memory()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "ebDbBjfFViIh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}